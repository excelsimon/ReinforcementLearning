# Lesson 3 ç¥ç»ç½‘ç»œæ–¹æ³•æ±‚è§£RLâ€”â€”DQN
## 1. DQNç®€ä»‹
* ä¸ŠèŠ‚è¯¾ä»‹ç»çš„è¡¨æ ¼å‹æ–¹æ³•å­˜å‚¨çš„çŠ¶æ€æ•°é‡æœ‰é™ï¼Œå½“é¢å¯¹å›´æ£‹æˆ–æœºå™¨äººæ§åˆ¶è¿™ç±»æœ‰æ•°ä¸æ¸…çš„çŠ¶æ€çš„ç¯å¢ƒæ—¶ï¼Œè¡¨æ ¼å‹æ–¹æ³•åœ¨å­˜å‚¨å’ŒæŸ¥æ‰¾æ•ˆç‡ä¸Šéƒ½å—å±€é™ï¼Œ`DQN`çš„æå‡ºè§£å†³äº†è¿™ä¸€å±€é™ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼æ›¿ä»£`Q`è¡¨æ ¼ã€‚
* æœ¬è´¨ä¸Š`DQN`è¿˜æ˜¯ä¸€ä¸ª`Q-learning`ç®—æ³•ï¼Œæ›´æ–°æ–¹å¼ä¸€è‡´ã€‚ä¸ºäº†æ›´å¥½çš„æ¢ç´¢ç¯å¢ƒï¼ŒåŒæ ·çš„ä¹Ÿé‡‡ç”¨`Îµ-greedy`æ–¹æ³•è®­ç»ƒã€‚
* åœ¨`Q-learning`çš„åŸºç¡€ä¸Šï¼Œ`DQN`æå‡ºäº†ä¸¤ä¸ªæŠ€å·§ä½¿å¾—`Q`ç½‘ç»œçš„æ›´æ–°è¿­ä»£æ›´ç¨³å®šã€‚
    * ç»éªŒå›æ”¾ `Experience Replay`ï¼šä¸»è¦è§£å†³æ ·æœ¬å…³è”æ€§å’Œåˆ©ç”¨æ•ˆç‡çš„é—®é¢˜ã€‚ä½¿ç”¨ä¸€ä¸ªç»éªŒæ± å­˜å‚¨å¤šæ¡ç»éªŒ`s,a,r,s'`ï¼Œå†ä»ä¸­éšæœºæŠ½å–ä¸€æ‰¹æ•°æ®é€å»è®­ç»ƒã€‚
    * å›ºå®šQç›®æ ‡ `Fixed-Q-Target`ï¼šä¸»è¦è§£å†³ç®—æ³•è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚å¤åˆ¶ä¸€ä¸ªå’ŒåŸæ¥`Q`ç½‘ç»œç»“æ„ä¸€æ ·çš„`Target Q`ç½‘ç»œï¼Œç”¨äºè®¡ç®—`Q`ç›®æ ‡å€¼ã€‚

## 2. DQNå®è·µ
* ä½¿ç”¨`DQN`è§£å†³CartPoleé—®é¢˜ï¼Œç§»åŠ¨å°è½¦ä½¿å¾—è½¦ä¸Šçš„æ‘†æ†å€’ç«‹èµ·æ¥ã€‚

### Step1 å®‰è£…ä¾èµ–


```python
!pip uninstall -y parl  # è¯´æ˜ï¼šAIStudioé¢„è£…çš„parlç‰ˆæœ¬å¤ªè€ï¼Œå®¹æ˜“è·Ÿå…¶ä»–åº“äº§ç”Ÿå…¼å®¹æ€§å†²çªï¼Œå»ºè®®å…ˆå¸è½½
!pip uninstall -y pandas scikit-learn # æç¤ºï¼šåœ¨AIStudioä¸­å¸è½½è¿™ä¸¤ä¸ªåº“å†import parlå¯é¿å…warningæç¤ºï¼Œä¸å¸è½½ä¹Ÿä¸å½±å“parlçš„ä½¿ç”¨

!pip install gym
!pip install paddlepaddle==1.6.3
!pip install parl==1.3.1

# è¯´æ˜ï¼šå®‰è£…æ—¥å¿—ä¸­å‡ºç°ä¸¤æ¡çº¢è‰²çš„å…³äº paddlehub å’Œ visualdl çš„ ERROR ä¸parlæ— å…³ï¼Œå¯ä»¥å¿½ç•¥ï¼Œä¸å½±å“ä½¿ç”¨
```

### Step2  å¯¼å…¥ä¾èµ–


```python
import parl
from parl import layers
import paddle.fluid as fluid
import copy
import numpy as np
import os
import gym
from parl.utils import logger
```

### Step3 è®¾ç½®è¶…å‚æ•°


```python
LEARN_FREQ = 5 # è®­ç»ƒé¢‘ç‡ï¼Œä¸éœ€è¦æ¯ä¸€ä¸ªstepéƒ½learnï¼Œæ”’ä¸€äº›æ–°å¢ç»éªŒåå†learnï¼Œæé«˜æ•ˆç‡
MEMORY_SIZE = 20000    # replay memoryçš„å¤§å°ï¼Œè¶Šå¤§è¶Šå ç”¨å†…å­˜
MEMORY_WARMUP_SIZE = 200  # replay_memory é‡Œéœ€è¦é¢„å­˜ä¸€äº›ç»éªŒæ•°æ®ï¼Œå†å¼€å¯è®­ç»ƒ
BATCH_SIZE = 32   # æ¯æ¬¡ç»™agent learnçš„æ•°æ®æ•°é‡ï¼Œä»replay memoryéšæœºé‡Œsampleä¸€æ‰¹æ•°æ®å‡ºæ¥
LEARNING_RATE = 0.001 # å­¦ä¹ ç‡
GAMMA = 0.99 # reward çš„è¡°å‡å› å­ï¼Œä¸€èˆ¬å– 0.9 åˆ° 0.999 ä¸ç­‰
```

### Step4 æ­å»ºModelã€Algorithmã€Agentæ¶æ„
* `Agent`æŠŠäº§ç”Ÿçš„æ•°æ®ä¼ ç»™`algorithm`ï¼Œ`algorithm`æ ¹æ®`model`çš„æ¨¡å‹ç»“æ„è®¡ç®—å‡º`Loss`ï¼Œä½¿ç”¨`SGD`æˆ–è€…å…¶ä»–ä¼˜åŒ–å™¨ä¸æ–­çš„ä¼˜åŒ–ï¼Œ`PARL`è¿™ç§æ¶æ„å¯ä»¥å¾ˆæ–¹ä¾¿çš„åº”ç”¨åœ¨å„ç±»æ·±åº¦å¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸­ã€‚

#### ï¼ˆ1ï¼‰Model
* `Model`ç”¨æ¥å®šä¹‰å‰å‘(`Forward`)ç½‘ç»œï¼Œç”¨æˆ·å¯ä»¥è‡ªç”±çš„å®šåˆ¶è‡ªå·±çš„ç½‘ç»œç»“æ„ã€‚


```python
class Model(parl.Model):
    def __init__(self, act_dim):
        hid1_size = 128
        hid2_size = 128
        # 3å±‚å…¨è¿æ¥ç½‘ç»œ
        self.fc1 = layers.fc(size=hid1_size, act='relu')
        self.fc2 = layers.fc(size=hid2_size, act='relu')
        self.fc3 = layers.fc(size=act_dim, act=None)

    def value(self, obs):
        # å®šä¹‰ç½‘ç»œ
        # è¾“å…¥stateï¼Œè¾“å‡ºæ‰€æœ‰actionå¯¹åº”çš„Qï¼Œ[Q(s,a1), Q(s,a2), Q(s,a3)...]
        h1 = self.fc1(obs)
        h2 = self.fc2(h1)
        Q = self.fc3(h2)
        return Q
```

#### ï¼ˆ2ï¼‰Algorithm
* `Algorithm`Â å®šä¹‰äº†å…·ä½“çš„ç®—æ³•æ¥æ›´æ–°å‰å‘ç½‘ç»œ(`Model`)ï¼Œä¹Ÿå°±æ˜¯é€šè¿‡å®šä¹‰æŸå¤±å‡½æ•°æ¥æ›´æ–°`Model`ï¼Œå’Œç®—æ³•ç›¸å…³çš„è®¡ç®—éƒ½æ”¾åœ¨`algorithm`ä¸­ã€‚


```python
# from parl.algorithms import DQN # ä¹Ÿå¯ä»¥ç›´æ¥ä»parlåº“ä¸­å¯¼å…¥DQNç®—æ³•

class DQN(parl.Algorithm):
    def __init__(self, model, act_dim=None, gamma=None, lr=None):
        """ DQN algorithm
        
        Args:
            model (parl.Model): å®šä¹‰Qå‡½æ•°çš„å‰å‘ç½‘ç»œç»“æ„
            act_dim (int): actionç©ºé—´çš„ç»´åº¦ï¼Œå³æœ‰å‡ ä¸ªaction
            gamma (float): rewardçš„è¡°å‡å› å­
            lr (float): learning rate å­¦ä¹ ç‡.
        """
        self.model = model
        self.target_model = copy.deepcopy(model)

        assert isinstance(act_dim, int)
        assert isinstance(gamma, float)
        assert isinstance(lr, float)
        self.act_dim = act_dim
        self.gamma = gamma
        self.lr = lr

    def predict(self, obs):
        """ ä½¿ç”¨self.modelçš„valueç½‘ç»œæ¥è·å– [Q(s,a1),Q(s,a2),...]
        """
        return self.model.value(obs)

    def learn(self, obs, action, reward, next_obs, terminal):
        """ ä½¿ç”¨DQNç®—æ³•æ›´æ–°self.modelçš„valueç½‘ç»œ
        """
        # ä»target_modelä¸­è·å– max Q' çš„å€¼ï¼Œç”¨äºè®¡ç®—target_Q
        next_pred_value = self.target_model.value(next_obs)
        best_v = layers.reduce_max(next_pred_value, dim=1)
        best_v.stop_gradient = True  # é˜»æ­¢æ¢¯åº¦ä¼ é€’
        terminal = layers.cast(terminal, dtype='float32')
        target = reward + (1.0 - terminal) * self.gamma * best_v

        pred_value = self.model.value(obs)  # è·å–Qé¢„æµ‹å€¼
        # å°†actionè½¬onehotå‘é‡ï¼Œæ¯”å¦‚ï¼š3 => [0,0,0,1,0]
        action_onehot = layers.one_hot(action, self.act_dim)
        action_onehot = layers.cast(action_onehot, dtype='float32')
        # ä¸‹é¢ä¸€è¡Œæ˜¯é€å…ƒç´ ç›¸ä¹˜ï¼Œæ‹¿åˆ°actionå¯¹åº”çš„ Q(s,a)
        # æ¯”å¦‚ï¼špred_value = [[2.3, 5.7, 1.2, 3.9, 1.4]], action_onehot = [[0,0,0,1,0]]
        #  ==> pred_action_value = [[3.9]]
        pred_action_value = layers.reduce_sum(
            layers.elementwise_mul(action_onehot, pred_value), dim=1)

        # è®¡ç®— Q(s,a) ä¸ target_Qçš„å‡æ–¹å·®ï¼Œå¾—åˆ°loss
        cost = layers.square_error_cost(pred_action_value, target)
        cost = layers.reduce_mean(cost)
        optimizer = fluid.optimizer.Adam(learning_rate=self.lr)  # ä½¿ç”¨Adamä¼˜åŒ–å™¨
        optimizer.minimize(cost)
        return cost

    def sync_target(self):
        """ æŠŠ self.model çš„æ¨¡å‹å‚æ•°å€¼åŒæ­¥åˆ° self.target_model
        """
        self.model.sync_weights_to(self.target_model)

```

#### ï¼ˆ3ï¼‰Agent
* `Agent`Â è´Ÿè´£ç®—æ³•ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œåœ¨äº¤äº’è¿‡ç¨‹ä¸­æŠŠç”Ÿæˆçš„æ•°æ®æä¾›ç»™`Algorithm`æ¥æ›´æ–°æ¨¡å‹(`Model`)ï¼Œæ•°æ®çš„é¢„å¤„ç†æµç¨‹ä¹Ÿä¸€èˆ¬å®šä¹‰åœ¨è¿™é‡Œã€‚


```python
class Agent(parl.Agent):
    def __init__(self,
                 algorithm,
                 obs_dim,
                 act_dim,
                 e_greed=0.1,
                 e_greed_decrement=0):
        assert isinstance(obs_dim, int)
        assert isinstance(act_dim, int)
        self.obs_dim = obs_dim
        self.act_dim = act_dim
        super(Agent, self).__init__(algorithm)

        self.global_step = 0
        self.update_target_steps = 200  # æ¯éš”200ä¸ªtraining stepså†æŠŠmodelçš„å‚æ•°å¤åˆ¶åˆ°target_modelä¸­

        self.e_greed = e_greed  # æœ‰ä¸€å®šæ¦‚ç‡éšæœºé€‰å–åŠ¨ä½œï¼Œæ¢ç´¢
        self.e_greed_decrement = e_greed_decrement  # éšç€è®­ç»ƒé€æ­¥æ”¶æ•›ï¼Œæ¢ç´¢çš„ç¨‹åº¦æ…¢æ…¢é™ä½

    def build_program(self):
        self.pred_program = fluid.Program()
        self.learn_program = fluid.Program()

        with fluid.program_guard(self.pred_program):  # æ­å»ºè®¡ç®—å›¾ç”¨äº é¢„æµ‹åŠ¨ä½œï¼Œå®šä¹‰è¾“å…¥è¾“å‡ºå˜é‡
            obs = layers.data(
                name='obs', shape=[self.obs_dim], dtype='float32')
            self.value = self.alg.predict(obs)

        with fluid.program_guard(self.learn_program):  # æ­å»ºè®¡ç®—å›¾ç”¨äº æ›´æ–°Qç½‘ç»œï¼Œå®šä¹‰è¾“å…¥è¾“å‡ºå˜é‡
            obs = layers.data(
                name='obs', shape=[self.obs_dim], dtype='float32')
            action = layers.data(name='act', shape=[1], dtype='int32')
            reward = layers.data(name='reward', shape=[], dtype='float32')
            next_obs = layers.data(
                name='next_obs', shape=[self.obs_dim], dtype='float32')
            terminal = layers.data(name='terminal', shape=[], dtype='bool')
            self.cost = self.alg.learn(obs, action, reward, next_obs, terminal)

    def sample(self, obs):
        sample = np.random.rand()  # äº§ç”Ÿ0~1ä¹‹é—´çš„å°æ•°
        if sample < self.e_greed:
            act = np.random.randint(self.act_dim)  # æ¢ç´¢ï¼šæ¯ä¸ªåŠ¨ä½œéƒ½æœ‰æ¦‚ç‡è¢«é€‰æ‹©
        else:
            act = self.predict(obs)  # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        self.e_greed = max(
            0.01, self.e_greed - self.e_greed_decrement)  # éšç€è®­ç»ƒé€æ­¥æ”¶æ•›ï¼Œæ¢ç´¢çš„ç¨‹åº¦æ…¢æ…¢é™ä½
        return act

    def predict(self, obs):  # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        obs = np.expand_dims(obs, axis=0)
        pred_Q = self.fluid_executor.run(
            self.pred_program,
            feed={'obs': obs.astype('float32')},
            fetch_list=[self.value])[0]
        pred_Q = np.squeeze(pred_Q, axis=0)
        act = np.argmax(pred_Q)  # é€‰æ‹©Qæœ€å¤§çš„ä¸‹æ ‡ï¼Œå³å¯¹åº”çš„åŠ¨ä½œ
        return act

    def learn(self, obs, act, reward, next_obs, terminal):
        # æ¯éš”200ä¸ªtraining stepsåŒæ­¥ä¸€æ¬¡modelå’Œtarget_modelçš„å‚æ•°
        if self.global_step % self.update_target_steps == 0:
            self.alg.sync_target()
        self.global_step += 1

        act = np.expand_dims(act, -1)
        feed = {
            'obs': obs.astype('float32'),
            'act': act.astype('int32'),
            'reward': reward,
            'next_obs': next_obs.astype('float32'),
            'terminal': terminal
        }
        cost = self.fluid_executor.run(
            self.learn_program, feed=feed, fetch_list=[self.cost])[0]  # è®­ç»ƒä¸€æ¬¡ç½‘ç»œ
        return cost
```

### Step5 ReplayMemory
* ç»éªŒæ± ï¼šç”¨äºå­˜å‚¨å¤šæ¡ç»éªŒï¼Œå®ç° ç»éªŒå›æ”¾ã€‚


```python
import random
import collections
import numpy as np


class ReplayMemory(object):
    def __init__(self, max_size):
        self.buffer = collections.deque(maxlen=max_size)

    # å¢åŠ ä¸€æ¡ç»éªŒåˆ°ç»éªŒæ± ä¸­
    def append(self, exp):
        self.buffer.append(exp)

    # ä»ç»éªŒæ± ä¸­é€‰å–Næ¡ç»éªŒå‡ºæ¥
    def sample(self, batch_size):
        mini_batch = random.sample(self.buffer, batch_size)
        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = [], [], [], [], []

        for experience in mini_batch:
            s, a, r, s_p, done = experience
            obs_batch.append(s)
            action_batch.append(a)
            reward_batch.append(r)
            next_obs_batch.append(s_p)
            done_batch.append(done)

        return np.array(obs_batch).astype('float32'), \
            np.array(action_batch).astype('float32'), np.array(reward_batch).astype('float32'),\
            np.array(next_obs_batch).astype('float32'), np.array(done_batch).astype('float32')

    def __len__(self):
        return len(self.buffer)

```

### Step6 Training && Testï¼ˆè®­ç»ƒ&&æµ‹è¯•ï¼‰


```python
# è®­ç»ƒä¸€ä¸ªepisode
def run_episode(env, agent, rpm):
    total_reward = 0
    obs = env.reset()
    step = 0
    while True:
        step += 1
        action = agent.sample(obs)  # é‡‡æ ·åŠ¨ä½œï¼Œæ‰€æœ‰åŠ¨ä½œéƒ½æœ‰æ¦‚ç‡è¢«å°è¯•åˆ°
        next_obs, reward, done, _ = env.step(action)
        rpm.append((obs, action, reward, next_obs, done))

        # train model
        if (len(rpm) > MEMORY_WARMUP_SIZE) and (step % LEARN_FREQ == 0):
            (batch_obs, batch_action, batch_reward, batch_next_obs,
             batch_done) = rpm.sample(BATCH_SIZE)
            train_loss = agent.learn(batch_obs, batch_action, batch_reward,
                                     batch_next_obs,
                                     batch_done)  # s,a,r,s',done

        total_reward += reward
        obs = next_obs
        if done:
            break
    return total_reward


# è¯„ä¼° agent, è·‘ 5 ä¸ªepisodeï¼Œæ€»rewardæ±‚å¹³å‡
def evaluate(env, agent, render=False):
    eval_reward = []
    for i in range(5):
        obs = env.reset()
        episode_reward = 0
        while True:
            action = agent.predict(obs)  # é¢„æµ‹åŠ¨ä½œï¼Œåªé€‰æœ€ä¼˜åŠ¨ä½œ
            obs, reward, done, _ = env.step(action)
            episode_reward += reward
            if render:
                env.render()
            if done:
                break
        eval_reward.append(episode_reward)
    return np.mean(eval_reward)

```

### Step7 åˆ›å»ºç¯å¢ƒå’ŒAgentï¼Œåˆ›å»ºç»éªŒæ± ï¼Œå¯åŠ¨è®­ç»ƒï¼Œä¿å­˜æ¨¡å‹


```python
env = gym.make('CartPole-v0')  # CartPole-v0: é¢„æœŸæœ€åä¸€æ¬¡è¯„ä¼°æ€»åˆ† > 180ï¼ˆæœ€å¤§å€¼æ˜¯200ï¼‰
action_dim = env.action_space.n  # CartPole-v0: 2
obs_shape = env.observation_space.shape  # CartPole-v0: (4,)

rpm = ReplayMemory(MEMORY_SIZE)  # DQNçš„ç»éªŒå›æ”¾æ± 

# æ ¹æ®parlæ¡†æ¶æ„å»ºagent
model = Model(act_dim=action_dim)
algorithm = DQN(model, act_dim=action_dim, gamma=GAMMA, lr=LEARNING_RATE)
agent = Agent(
    algorithm,
    obs_dim=obs_shape[0],
    act_dim=action_dim,
    e_greed=0.1,  # æœ‰ä¸€å®šæ¦‚ç‡éšæœºé€‰å–åŠ¨ä½œï¼Œæ¢ç´¢
    e_greed_decrement=1e-6)  # éšç€è®­ç»ƒé€æ­¥æ”¶æ•›ï¼Œæ¢ç´¢çš„ç¨‹åº¦æ…¢æ…¢é™ä½

# åŠ è½½æ¨¡å‹
# save_path = './dqn_model.ckpt'
# agent.restore(save_path)

# å…ˆå¾€ç»éªŒæ± é‡Œå­˜ä¸€äº›æ•°æ®ï¼Œé¿å…æœ€å¼€å§‹è®­ç»ƒçš„æ—¶å€™æ ·æœ¬ä¸°å¯Œåº¦ä¸å¤Ÿ
while len(rpm) < MEMORY_WARMUP_SIZE:
    run_episode(env, agent, rpm)

max_episode = 2000

# å¼€å§‹è®­ç»ƒ
episode = 0
while episode < max_episode:  # è®­ç»ƒmax_episodeä¸ªå›åˆï¼Œtestéƒ¨åˆ†ä¸è®¡ç®—å…¥episodeæ•°é‡
    # train part
    for i in range(0, 50):
        total_reward = run_episode(env, agent, rpm)
        episode += 1

    # test part
    eval_reward = evaluate(env, agent, render=False)  # render=True æŸ¥çœ‹æ˜¾ç¤ºæ•ˆæœ
    logger.info('episode:{}    e_greed:{}   test_reward:{}'.format(
        episode, agent.e_greed, eval_reward))

# è®­ç»ƒç»“æŸï¼Œä¿å­˜æ¨¡å‹
save_path = './dqn_model.ckpt'
agent.save(save_path)
```

    [32m[06-09 16:10:33 MainThread @machine_info.py:84][0m Cannot find available GPU devices, using CPU now.
    [32m[06-09 16:10:33 MainThread @machine_info.py:84][0m Cannot find available GPU devices, using CPU now.
    [32m[06-09 16:10:33 MainThread @machine_info.py:84][0m Cannot find available GPU devices, using CPU now.
    [32m[06-09 16:10:35 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:50    e_greed:0.09926899999999927   test_reward:9.6
    [32m[06-09 16:10:37 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:100    e_greed:0.09872899999999873   test_reward:12.4
    [32m[06-09 16:10:40 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:150    e_greed:0.0981919999999982   test_reward:11.4
    [32m[06-09 16:10:42 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:200    e_greed:0.09762199999999763   test_reward:9.6
    [32m[06-09 16:10:44 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:250    e_greed:0.09710399999999711   test_reward:9.2
    [32m[06-09 16:10:45 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:300    e_greed:0.09655699999999656   test_reward:10.2
    [32m[06-09 16:10:47 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:350    e_greed:0.09595199999999596   test_reward:10.0
    [32m[06-09 16:10:50 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:400    e_greed:0.09522299999999523   test_reward:9.4
    [32m[06-09 16:11:05 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:450    e_greed:0.09127799999999128   test_reward:119.6
    [32m[06-09 16:11:40 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:500    e_greed:0.08295799999998296   test_reward:200.0
    [32m[06-09 16:12:23 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:550    e_greed:0.07321999999997322   test_reward:161.2
    [32m[06-09 16:13:06 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:600    e_greed:0.06383999999996384   test_reward:172.6
    [32m[06-09 16:13:46 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:650    e_greed:0.054751999999954754   test_reward:139.2
    [32m[06-09 16:14:21 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:700    e_greed:0.04665099999994665   test_reward:127.6
    [32m[06-09 16:14:55 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:750    e_greed:0.03920699999993921   test_reward:134.2
    [32m[06-09 16:15:28 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:800    e_greed:0.03157199999993157   test_reward:181.2
    [32m[06-09 16:16:06 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:850    e_greed:0.023422999999923422   test_reward:114.4
    [32m[06-09 16:16:46 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:900    e_greed:0.015071999999916031   test_reward:193.8
    [32m[06-09 16:17:26 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:950    e_greed:0.01   test_reward:158.6
    [32m[06-09 16:17:55 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1000    e_greed:0.01   test_reward:175.4
    [32m[06-09 16:18:28 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1050    e_greed:0.01   test_reward:136.8
    [32m[06-09 16:18:55 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1100    e_greed:0.01   test_reward:140.6
    [32m[06-09 16:19:30 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1150    e_greed:0.01   test_reward:76.4
    [32m[06-09 16:19:59 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1200    e_greed:0.01   test_reward:195.0
    [32m[06-09 16:20:39 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1250    e_greed:0.01   test_reward:200.0
    [32m[06-09 16:21:25 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1300    e_greed:0.01   test_reward:200.0
    [32m[06-09 16:22:06 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1350    e_greed:0.01   test_reward:131.2
    [32m[06-09 16:22:51 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1400    e_greed:0.01   test_reward:193.2
    [32m[06-09 16:23:36 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1450    e_greed:0.01   test_reward:200.0
    [32m[06-09 16:24:17 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1500    e_greed:0.01   test_reward:200.0
    [32m[06-09 16:24:58 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1550    e_greed:0.01   test_reward:200.0
    [32m[06-09 16:25:42 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1600    e_greed:0.01   test_reward:200.0
    [32m[06-09 16:26:09 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1650    e_greed:0.01   test_reward:70.4
    [32m[06-09 16:26:20 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1700    e_greed:0.01   test_reward:118.0
    [32m[06-09 16:27:03 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1750    e_greed:0.01   test_reward:200.0
    [32m[06-09 16:27:49 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1800    e_greed:0.01   test_reward:200.0
    [32m[06-09 16:28:35 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1850    e_greed:0.01   test_reward:200.0
    [32m[06-09 16:29:08 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1900    e_greed:0.01   test_reward:114.2
    [32m[06-09 16:29:40 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:1950    e_greed:0.01   test_reward:200.0
    [32m[06-09 16:30:24 MainThread @<ipython-input-14-1f01c261665f>:39][0m episode:2000    e_greed:0.01   test_reward:200.0

