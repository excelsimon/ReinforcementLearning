{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lesson 5 连续动作空间上求解RL——DDPG\n",
    "## 1. DDPG简介\n",
    "* `DDPG`的提出动机其实是为了让`DQN`可以扩展到连续的动作空间。\n",
    "* `DDPG`借鉴了`DQN`的两个技巧：经验回放 和 固定`Q`网络。\n",
    "* `DDPG`使用策略网络直接输出确定性动作。\n",
    "* `DDPG`使用了`Actor-Critic`的架构。\n",
    "\n",
    "## 2. DDPG实践\n",
    "* 使用`DDPG`解决连续控制版本的`CartPole`问题，给小车一个力（连续量）使得车上的摆杆倒立起来。\n",
    "\n",
    "### Step1 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y parl  # 说明：AIStudio预装的parl版本太老，容易跟其他库产生兼容性冲突，建议先卸载\n",
    "!pip uninstall -y pandas scikit-learn # 提示：在AIStudio中卸载这两个库再import parl可避免warning提示，不卸载也不影响parl的使用\n",
    "\n",
    "!pip install gym\n",
    "!pip install paddlepaddle==1.6.3\n",
    "!pip install parl==1.3.1\n",
    "\n",
    "# 说明：安装日志中出现两条红色的关于 paddlehub 和 visualdl 的 ERROR 与parl无关，可以忽略，不影响使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 检查依赖包版本是否正确\n",
    "!pip list | grep paddlepaddle\n",
    "!pip list | grep parl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step2 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "import parl\n",
    "from parl import layers\n",
    "from parl.utils import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step3 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ACTOR_LR = 1e-3  # Actor网络的 learning rate\n",
    "CRITIC_LR = 1e-3  # Critic网络的 learning rate\n",
    "\n",
    "GAMMA = 0.99      # reward 的衰减因子\n",
    "TAU = 0.001       # 软更新的系数\n",
    "MEMORY_SIZE = int(1e6)                  # 经验池大小\n",
    "MEMORY_WARMUP_SIZE = MEMORY_SIZE // 20  # 预存一部分经验之后再开始训练\n",
    "BATCH_SIZE = 128\n",
    "REWARD_SCALE = 0.1   # reward 缩放系数\n",
    "NOISE = 0.05         # 动作噪声方差\n",
    "\n",
    "TRAIN_EPISODE = 6000 # 训练的总episode数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step4 搭建Model、Algorithm、Agent架构\n",
    "* `Agent`把产生的数据传给`algorithm`，`algorithm`根据`model`的模型结构计算出`Loss`，使用`SGD`或者其他优化器不断的优化，`PARL`这种架构可以很方便的应用在各类深度强化学习问题中。\n",
    "#### （1）Model\n",
    "`Model`用来定义前向(`Forward`)网络，用户可以自由的定制自己的网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        self.actor_model = ActorModel(act_dim)\n",
    "        self.critic_model = CriticModel()\n",
    "\n",
    "    def policy(self, obs):\n",
    "        return self.actor_model.policy(obs)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        return self.critic_model.value(obs, act)\n",
    "\n",
    "    def get_actor_params(self):\n",
    "        return self.actor_model.parameters()\n",
    "\n",
    "\n",
    "class ActorModel(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        hid_size = 100\n",
    "\n",
    "        self.fc1 = layers.fc(size=hid_size, act='relu')\n",
    "        self.fc2 = layers.fc(size=act_dim, act='tanh')\n",
    "\n",
    "    def policy(self, obs):\n",
    "        hid = self.fc1(obs)\n",
    "        means = self.fc2(hid)\n",
    "        return means\n",
    "\n",
    "\n",
    "class CriticModel(parl.Model):\n",
    "    def __init__(self):\n",
    "        hid_size = 100\n",
    "\n",
    "        self.fc1 = layers.fc(size=hid_size, act='relu')\n",
    "        self.fc2 = layers.fc(size=1, act=None)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        concat = layers.concat([obs, act], axis=1)\n",
    "        hid = self.fc1(concat)\n",
    "        Q = self.fc2(hid)\n",
    "        Q = layers.squeeze(Q, axes=[1])\n",
    "        return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### （2）Algorithm\n",
    "* `Algorithm` 定义了具体的算法来更新前向网络(`Model`)，也就是通过定义损失函数来更新`Model`，和算法相关的计算都放在`algorithm`中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from parl.algorithms import DDPG # 也可以直接从parl库中快速引入DDPG算法，无需自己重新写算法\n",
    "\n",
    "class DDPG(parl.Algorithm):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 gamma=None,\n",
    "                 tau=None,\n",
    "                 actor_lr=None,\n",
    "                 critic_lr=None):\n",
    "        \"\"\"  DDPG algorithm\n",
    "        \n",
    "        Args:\n",
    "            model (parl.Model): actor and critic 的前向网络.\n",
    "                                model 必须实现 get_actor_params() 方法.\n",
    "            gamma (float): reward的衰减因子.\n",
    "            tau (float): self.target_model 跟 self.model 同步参数 的 软更新参数\n",
    "            actor_lr (float): actor 的学习率\n",
    "            critic_lr (float): critic 的学习率\n",
    "        \"\"\"\n",
    "        assert isinstance(gamma, float)\n",
    "        assert isinstance(tau, float)\n",
    "        assert isinstance(actor_lr, float)\n",
    "        assert isinstance(critic_lr, float)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "\n",
    "        self.model = model\n",
    "        self.target_model = deepcopy(model)\n",
    "\n",
    "    def predict(self, obs):\n",
    "        \"\"\" 使用 self.model 的 actor model 来预测动作\n",
    "        \"\"\"\n",
    "        return self.model.policy(obs)\n",
    "\n",
    "    def learn(self, obs, action, reward, next_obs, terminal):\n",
    "        \"\"\" 用DDPG算法更新 actor 和 critic\n",
    "        \"\"\"\n",
    "        actor_cost = self._actor_learn(obs)\n",
    "        critic_cost = self._critic_learn(obs, action, reward, next_obs,\n",
    "                                         terminal)\n",
    "        return actor_cost, critic_cost\n",
    "\n",
    "    def _actor_learn(self, obs):\n",
    "        action = self.model.policy(obs)\n",
    "        Q = self.model.value(obs, action)\n",
    "        cost = layers.reduce_mean(-1.0 * Q)\n",
    "        optimizer = fluid.optimizer.AdamOptimizer(self.actor_lr)\n",
    "        optimizer.minimize(cost, parameter_list=self.model.get_actor_params())\n",
    "        return cost\n",
    "\n",
    "    def _critic_learn(self, obs, action, reward, next_obs, terminal):\n",
    "        next_action = self.target_model.policy(next_obs)\n",
    "        next_Q = self.target_model.value(next_obs, next_action)\n",
    "\n",
    "        terminal = layers.cast(terminal, dtype='float32')\n",
    "        target_Q = reward + (1.0 - terminal) * self.gamma * next_Q\n",
    "        target_Q.stop_gradient = True\n",
    "\n",
    "        Q = self.model.value(obs, action)\n",
    "        cost = layers.square_error_cost(Q, target_Q)\n",
    "        cost = layers.reduce_mean(cost)\n",
    "        optimizer = fluid.optimizer.AdamOptimizer(self.critic_lr)\n",
    "        optimizer.minimize(cost)\n",
    "        return cost\n",
    "\n",
    "    def sync_target(self, decay=None, share_vars_parallel_executor=None):\n",
    "        \"\"\" self.target_model从self.model复制参数过来，可设置软更新参数\n",
    "        \"\"\"\n",
    "        if decay is None:\n",
    "            decay = 1.0 - self.tau\n",
    "        self.model.sync_weights_to(\n",
    "            self.target_model,\n",
    "            decay=decay,\n",
    "            share_vars_parallel_executor=share_vars_parallel_executor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### （3）Agent\n",
    "* `Agent`负责算法与环境的交互，在交互过程中把生成的数据提供给`Algorithm`来更新模型(`Model`)，数据的预处理流程也一般定义在这里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent(parl.Agent):\n",
    "    def __init__(self, algorithm, obs_dim, act_dim):\n",
    "        assert isinstance(obs_dim, int)\n",
    "        assert isinstance(act_dim, int)\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(Agent, self).__init__(algorithm)\n",
    "\n",
    "        # 注意：最开始先同步self.model和self.target_model的参数.\n",
    "        self.alg.sync_target(decay=0)\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.pred_act = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(self.learn_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            act = layers.data(\n",
    "                name='act', shape=[self.act_dim], dtype='float32')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            next_obs = layers.data(\n",
    "                name='next_obs', shape=[self.obs_dim], dtype='float32')\n",
    "            terminal = layers.data(name='terminal', shape=[], dtype='bool')\n",
    "            _, self.critic_cost = self.alg.learn(obs, act, reward, next_obs,\n",
    "                                                 terminal)\n",
    "\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act = self.fluid_executor.run(\n",
    "            self.pred_program, feed={'obs': obs},\n",
    "            fetch_list=[self.pred_act])[0]\n",
    "        act = np.squeeze(act)\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward, next_obs, terminal):\n",
    "        feed = {\n",
    "            'obs': obs,\n",
    "            'act': act,\n",
    "            'reward': reward,\n",
    "            'next_obs': next_obs,\n",
    "            'terminal': terminal\n",
    "        }\n",
    "        critic_cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.critic_cost])[0]\n",
    "        self.alg.sync_target()\n",
    "        return critic_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### env.py\n",
    "#### 连续控制版本的CartPole环境 \n",
    "* 该环境代码与算法无关，可忽略不看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# env.py\n",
    "# Continuous version of Cartpole\n",
    "\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ContinuousCartPoleEnv(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 30.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.min_action = -1.0\n",
    "        self.max_action = 1.0\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds\n",
    "        high = np.array([\n",
    "            self.x_threshold * 2,\n",
    "            np.finfo(np.float32).max,\n",
    "            self.theta_threshold_radians * 2,\n",
    "            np.finfo(np.float32).max])\n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "            low=self.min_action,\n",
    "            high=self.max_action,\n",
    "            shape=(1,)\n",
    "        )\n",
    "        self.observation_space = spaces.Box(-high, high)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def stepPhysics(self, force):\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / \\\n",
    "            (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "        return (x, x_dot, theta, theta_dot)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.expand_dims(action, 0)\n",
    "        assert self.action_space.contains(action), \\\n",
    "            \"%r (%s) invalid\" % (action, type(action))\n",
    "        # Cast action to float to strip np trappings\n",
    "        force = self.force_mag * float(action)\n",
    "        self.state = self.stepPhysics(force)\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        done = x < -self.x_threshold \\\n",
    "            or x > self.x_threshold \\\n",
    "            or theta < -self.theta_threshold_radians \\\n",
    "            or theta > self.theta_threshold_radians\n",
    "        done = bool(done)\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                gym.logger.warn(\"\"\"\n",
    "You are calling 'step()' even though this environment has already returned\n",
    "done = True. You should always call 'reset()' once you receive 'done = True'\n",
    "Any further steps are undefined behavior.\n",
    "                \"\"\")\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width /world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * 1.0\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "            axleoffset = cartheight / 4.0\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l, r, t, b = -polewidth / 2, polewidth / 2, polelen-polewidth / 2, -polewidth / 2\n",
    "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            pole.set_color(.8, .6, .4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth / 2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5, .5, .8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## replay_memory.py\n",
    "#### 经验池 ReplayMemory\n",
    "* 与`DQN`的`replay_mamory.py`代码一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replay_memory.py\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = collections.deque(maxlen=max_size)\n",
    "\n",
    "    def append(self, exp):\n",
    "        self.buffer.append(exp)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        mini_batch = random.sample(self.buffer, batch_size)\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = [], [], [], [], []\n",
    "\n",
    "        for experience in mini_batch:\n",
    "            s, a, r, s_p, done = experience\n",
    "            obs_batch.append(s)\n",
    "            action_batch.append(a)\n",
    "            reward_batch.append(r)\n",
    "            next_obs_batch.append(s_p)\n",
    "            done_batch.append(done)\n",
    "\n",
    "        return np.array(obs_batch).astype('float32'), \\\n",
    "            np.array(action_batch).astype('float32'), np.array(reward_batch).astype('float32'),\\\n",
    "            np.array(next_obs_batch).astype('float32'), np.array(done_batch).astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step5 Training && Test（训练&&测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_episode(agent, env, rpm):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        steps += 1\n",
    "        batch_obs = np.expand_dims(obs, axis=0)\n",
    "        action = agent.predict(batch_obs.astype('float32'))\n",
    "\n",
    "        # 增加探索扰动, 输出限制在 [-1.0, 1.0] 范围内\n",
    "        action = np.clip(np.random.normal(action, NOISE), -1.0, 1.0)\n",
    "\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "        action = [action]  # 方便存入replaymemory\n",
    "        rpm.append((obs, action, REWARD_SCALE * reward, next_obs, done))\n",
    "\n",
    "        if len(rpm) > MEMORY_WARMUP_SIZE and (steps % 5) == 0:\n",
    "            (batch_obs, batch_action, batch_reward, batch_next_obs,\n",
    "             batch_done) = rpm.sample(BATCH_SIZE)\n",
    "            agent.learn(batch_obs, batch_action, batch_reward, batch_next_obs,\n",
    "                        batch_done)\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        if done or steps >= 200:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate(env, agent, render=False):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            batch_obs = np.expand_dims(obs, axis=0)\n",
    "            action = agent.predict(batch_obs.astype('float32'))\n",
    "            action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "            steps += 1\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done or steps >= 200:\n",
    "                break\n",
    "        eval_reward.append(total_reward)\n",
    "    return np.mean(eval_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step6 创建环境和Agent，创建经验池，启动训练，保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-11 16:26:59 MainThread @machine_info.py:84]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-11 16:26:59 MainThread @machine_info.py:84]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-11 16:26:59 MainThread @machine_info.py:84]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-11 16:28:44 MainThread @machine_info.py:84]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-11 16:28:46 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:50    test_reward:6.6\n",
      "\u001b[32m[06-11 16:28:48 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:100    test_reward:5.8\n",
      "\u001b[32m[06-11 16:28:49 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:150    test_reward:6.0\n",
      "\u001b[32m[06-11 16:28:50 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:200    test_reward:6.0\n",
      "\u001b[32m[06-11 16:28:52 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:250    test_reward:5.8\n",
      "\u001b[32m[06-11 16:28:53 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:300    test_reward:5.8\n",
      "\u001b[32m[06-11 16:28:55 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:350    test_reward:5.8\n",
      "\u001b[32m[06-11 16:28:56 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:400    test_reward:6.2\n",
      "\u001b[32m[06-11 16:28:58 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:450    test_reward:5.8\n",
      "\u001b[32m[06-11 16:28:59 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:500    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:01 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:550    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:02 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:600    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:04 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:650    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:05 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:700    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:06 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:750    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:08 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:800    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:09 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:850    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:11 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:900    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:12 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:950    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:14 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1000    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:15 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1050    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:17 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1100    test_reward:5.6\n",
      "\u001b[32m[06-11 16:29:18 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1150    test_reward:6.2\n",
      "\u001b[32m[06-11 16:29:19 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1200    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:21 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1250    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:22 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1300    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:24 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1350    test_reward:5.6\n",
      "\u001b[32m[06-11 16:29:25 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1400    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:27 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1450    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:28 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1500    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:30 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1550    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:31 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1600    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:32 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1650    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:34 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1700    test_reward:5.6\n",
      "\u001b[32m[06-11 16:29:35 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1750    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:37 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1800    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:39 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1850    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:40 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1900    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:42 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:1950    test_reward:5.6\n",
      "\u001b[32m[06-11 16:29:43 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2000    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:45 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2050    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:46 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2100    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:48 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2150    test_reward:5.6\n",
      "\u001b[32m[06-11 16:29:49 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2200    test_reward:5.6\n",
      "\u001b[32m[06-11 16:29:50 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2250    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:52 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2300    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:53 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2350    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:55 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2400    test_reward:5.8\n",
      "\u001b[32m[06-11 16:29:56 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2450    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:58 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2500    test_reward:6.0\n",
      "\u001b[32m[06-11 16:29:59 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2550    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:01 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2600    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:02 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2650    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:04 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2700    test_reward:5.4\n",
      "\u001b[32m[06-11 16:30:05 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2750    test_reward:6.0\n",
      "\u001b[32m[06-11 16:30:07 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2800    test_reward:6.0\n",
      "\u001b[32m[06-11 16:30:08 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2850    test_reward:6.0\n",
      "\u001b[32m[06-11 16:30:10 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2900    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:11 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:2950    test_reward:6.2\n",
      "\u001b[32m[06-11 16:30:13 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3000    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:14 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3050    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:16 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3100    test_reward:6.0\n",
      "\u001b[32m[06-11 16:30:17 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3150    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:19 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3200    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:20 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3250    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:22 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3300    test_reward:6.0\n",
      "\u001b[32m[06-11 16:30:23 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3350    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:25 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3400    test_reward:5.6\n",
      "\u001b[32m[06-11 16:30:26 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3450    test_reward:6.0\n",
      "\u001b[32m[06-11 16:30:28 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3500    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:29 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3550    test_reward:6.0\n",
      "\u001b[32m[06-11 16:30:31 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3600    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:32 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3650    test_reward:5.8\n",
      "\u001b[32m[06-11 16:30:34 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3700    test_reward:6.0\n",
      "\u001b[32m[06-11 16:30:36 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3750    test_reward:6.0\n",
      "\u001b[32m[06-11 16:30:37 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3800    test_reward:6.0\n",
      "\u001b[32m[06-11 16:30:39 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3850    test_reward:5.6\n",
      "\u001b[32m[06-11 16:30:40 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3900    test_reward:6.0\n",
      "\u001b[32m[06-11 16:30:42 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:3950    test_reward:5.6\n",
      "\u001b[32m[06-11 16:30:43 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4000    test_reward:5.6\n",
      "\u001b[32m[06-11 16:30:50 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4050    test_reward:56.0\n",
      "\u001b[32m[06-11 16:31:19 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4100    test_reward:200.0\n",
      "\u001b[32m[06-11 16:32:11 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4150    test_reward:200.0\n",
      "\u001b[32m[06-11 16:33:08 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4200    test_reward:200.0\n",
      "\u001b[32m[06-11 16:34:06 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4250    test_reward:200.0\n",
      "\u001b[32m[06-11 16:35:05 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4300    test_reward:200.0\n",
      "\u001b[32m[06-11 16:36:04 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4350    test_reward:200.0\n",
      "\u001b[32m[06-11 16:37:04 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4400    test_reward:200.0\n",
      "\u001b[32m[06-11 16:38:03 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4450    test_reward:200.0\n",
      "\u001b[32m[06-11 16:39:02 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4500    test_reward:200.0\n",
      "\u001b[32m[06-11 16:40:03 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4550    test_reward:200.0\n",
      "\u001b[32m[06-11 16:40:58 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4600    test_reward:141.2\n",
      "\u001b[32m[06-11 16:41:50 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4650    test_reward:200.0\n",
      "\u001b[32m[06-11 16:42:53 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4700    test_reward:200.0\n",
      "\u001b[32m[06-11 16:43:54 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4750    test_reward:200.0\n",
      "\u001b[32m[06-11 16:44:55 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4800    test_reward:200.0\n",
      "\u001b[32m[06-11 16:45:57 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4850    test_reward:200.0\n",
      "\u001b[32m[06-11 16:46:57 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4900    test_reward:200.0\n",
      "\u001b[32m[06-11 16:47:58 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:4950    test_reward:200.0\n",
      "\u001b[32m[06-11 16:48:57 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5000    test_reward:200.0\n",
      "\u001b[32m[06-11 16:49:58 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5050    test_reward:200.0\n",
      "\u001b[32m[06-11 16:50:56 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5100    test_reward:200.0\n",
      "\u001b[32m[06-11 16:52:03 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5150    test_reward:200.0\n",
      "\u001b[32m[06-11 16:53:07 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5200    test_reward:200.0\n",
      "\u001b[32m[06-11 16:54:15 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5250    test_reward:200.0\n",
      "\u001b[32m[06-11 16:55:18 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5300    test_reward:200.0\n",
      "\u001b[32m[06-11 16:56:28 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5350    test_reward:200.0\n",
      "\u001b[32m[06-11 16:57:34 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5400    test_reward:198.4\n",
      "\u001b[32m[06-11 16:58:43 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5450    test_reward:188.4\n",
      "\u001b[32m[06-11 16:59:51 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5500    test_reward:198.8\n",
      "\u001b[32m[06-11 17:00:58 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5550    test_reward:200.0\n",
      "\u001b[32m[06-11 17:02:10 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5600    test_reward:200.0\n",
      "\u001b[32m[06-11 17:03:18 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5650    test_reward:200.0\n",
      "\u001b[32m[06-11 17:04:31 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5700    test_reward:200.0\n",
      "\u001b[32m[06-11 17:05:44 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5750    test_reward:200.0\n",
      "\u001b[32m[06-11 17:06:53 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5800    test_reward:200.0\n",
      "\u001b[32m[06-11 17:08:09 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5850    test_reward:200.0\n",
      "\u001b[32m[06-11 17:09:23 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5900    test_reward:200.0\n",
      "\u001b[32m[06-11 17:10:32 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:5950    test_reward:200.0\n",
      "\u001b[32m[06-11 17:11:52 MainThread @<ipython-input-10-f4e0a11a1638>:27]\u001b[0m episode:6000    test_reward:200.0\n"
     ]
    }
   ],
   "source": [
    "# 创建环境\n",
    "env = ContinuousCartPoleEnv()\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "# 使用PARL框架创建agent\n",
    "model = Model(act_dim)\n",
    "algorithm = DDPG(\n",
    "    model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n",
    "agent = Agent(algorithm, obs_dim, act_dim)\n",
    "\n",
    "# 创建经验池\n",
    "rpm = ReplayMemory(MEMORY_SIZE)\n",
    "# 往经验池中预存数据\n",
    "while len(rpm) < MEMORY_WARMUP_SIZE:\n",
    "    run_episode(agent, env, rpm)\n",
    "\n",
    "episode = 0\n",
    "while episode < TRAIN_EPISODE:\n",
    "    for i in range(50):\n",
    "        total_reward = run_episode(agent, env, rpm)\n",
    "        episode += 1\n",
    "\n",
    "    eval_reward = evaluate(env, agent, render=False)\n",
    "    logger.info('episode:{}    test_reward:{}'.format(\n",
    "        episode, eval_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.7.1 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
