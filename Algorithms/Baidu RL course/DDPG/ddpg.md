# Lesson 5 è¿ç»­åŠ¨ä½œç©ºé—´ä¸Šæ±‚è§£RLâ€”â€”DDPG
## 1. DDPGç®€ä»‹
* `DDPG`çš„æå‡ºåŠ¨æœºå…¶å®æ˜¯ä¸ºäº†è®©`DQN`å¯ä»¥æ‰©å±•åˆ°è¿ç»­çš„åŠ¨ä½œç©ºé—´ã€‚
* `DDPG`å€Ÿé‰´äº†`DQN`çš„ä¸¤ä¸ªæŠ€å·§ï¼šç»éªŒå›æ”¾ å’Œ å›ºå®š`Q`ç½‘ç»œã€‚
* `DDPG`ä½¿ç”¨ç­–ç•¥ç½‘ç»œç›´æ¥è¾“å‡ºç¡®å®šæ€§åŠ¨ä½œã€‚
* `DDPG`ä½¿ç”¨äº†`Actor-Critic`çš„æ¶æ„ã€‚

## 2. DDPGå®è·µ
* ä½¿ç”¨`DDPG`è§£å†³è¿ç»­æ§åˆ¶ç‰ˆæœ¬çš„`CartPole`é—®é¢˜ï¼Œç»™å°è½¦ä¸€ä¸ªåŠ›ï¼ˆè¿ç»­é‡ï¼‰ä½¿å¾—è½¦ä¸Šçš„æ‘†æ†å€’ç«‹èµ·æ¥ã€‚

### Step1 å®‰è£…ä¾èµ–


```python
!pip uninstall -y parl  # è¯´æ˜ï¼šAIStudioé¢„è£…çš„parlç‰ˆæœ¬å¤ªè€ï¼Œå®¹æ˜“è·Ÿå…¶ä»–åº“äº§ç”Ÿå…¼å®¹æ€§å†²çªï¼Œå»ºè®®å…ˆå¸è½½
!pip uninstall -y pandas scikit-learn # æç¤ºï¼šåœ¨AIStudioä¸­å¸è½½è¿™ä¸¤ä¸ªåº“å†import parlå¯é¿å…warningæç¤ºï¼Œä¸å¸è½½ä¹Ÿä¸å½±å“parlçš„ä½¿ç”¨

!pip install gym
!pip install paddlepaddle==1.6.3
!pip install parl==1.3.1

# è¯´æ˜ï¼šå®‰è£…æ—¥å¿—ä¸­å‡ºç°ä¸¤æ¡çº¢è‰²çš„å…³äº paddlehub å’Œ visualdl çš„ ERROR ä¸parlæ— å…³ï¼Œå¯ä»¥å¿½ç•¥ï¼Œä¸å½±å“ä½¿ç”¨
```


```python
# æ£€æŸ¥ä¾èµ–åŒ…ç‰ˆæœ¬æ˜¯å¦æ­£ç¡®
!pip list | grep paddlepaddle
!pip list | grep parl
```

### Step2 å¯¼å…¥ä¾èµ–


```python
import gym
import numpy as np
from copy import deepcopy

import paddle.fluid as fluid
import parl
from parl import layers
from parl.utils import logger
```

### Step3 è®¾ç½®è¶…å‚æ•°


```python
ACTOR_LR = 1e-3  # Actorç½‘ç»œçš„ learning rate
CRITIC_LR = 1e-3  # Criticç½‘ç»œçš„ learning rate

GAMMA = 0.99      # reward çš„è¡°å‡å› å­
TAU = 0.001       # è½¯æ›´æ–°çš„ç³»æ•°
MEMORY_SIZE = int(1e6)                  # ç»éªŒæ± å¤§å°
MEMORY_WARMUP_SIZE = MEMORY_SIZE // 20  # é¢„å­˜ä¸€éƒ¨åˆ†ç»éªŒä¹‹åå†å¼€å§‹è®­ç»ƒ
BATCH_SIZE = 128
REWARD_SCALE = 0.1   # reward ç¼©æ”¾ç³»æ•°
NOISE = 0.05         # åŠ¨ä½œå™ªå£°æ–¹å·®

TRAIN_EPISODE = 6000 # è®­ç»ƒçš„æ€»episodeæ•°
```

### Step4 æ­å»ºModelã€Algorithmã€Agentæ¶æ„
* `Agent`æŠŠäº§ç”Ÿçš„æ•°æ®ä¼ ç»™`algorithm`ï¼Œ`algorithm`æ ¹æ®`model`çš„æ¨¡å‹ç»“æ„è®¡ç®—å‡º`Loss`ï¼Œä½¿ç”¨`SGD`æˆ–è€…å…¶ä»–ä¼˜åŒ–å™¨ä¸æ–­çš„ä¼˜åŒ–ï¼Œ`PARL`è¿™ç§æ¶æ„å¯ä»¥å¾ˆæ–¹ä¾¿çš„åº”ç”¨åœ¨å„ç±»æ·±åº¦å¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸­ã€‚
#### ï¼ˆ1ï¼‰Model
`Model`ç”¨æ¥å®šä¹‰å‰å‘(`Forward`)ç½‘ç»œï¼Œç”¨æˆ·å¯ä»¥è‡ªç”±çš„å®šåˆ¶è‡ªå·±çš„ç½‘ç»œç»“æ„


```python
class Model(parl.Model):
    def __init__(self, act_dim):
        self.actor_model = ActorModel(act_dim)
        self.critic_model = CriticModel()

    def policy(self, obs):
        return self.actor_model.policy(obs)

    def value(self, obs, act):
        return self.critic_model.value(obs, act)

    def get_actor_params(self):
        return self.actor_model.parameters()


class ActorModel(parl.Model):
    def __init__(self, act_dim):
        hid_size = 100

        self.fc1 = layers.fc(size=hid_size, act='relu')
        self.fc2 = layers.fc(size=act_dim, act='tanh')

    def policy(self, obs):
        hid = self.fc1(obs)
        means = self.fc2(hid)
        return means


class CriticModel(parl.Model):
    def __init__(self):
        hid_size = 100

        self.fc1 = layers.fc(size=hid_size, act='relu')
        self.fc2 = layers.fc(size=1, act=None)

    def value(self, obs, act):
        concat = layers.concat([obs, act], axis=1)
        hid = self.fc1(concat)
        Q = self.fc2(hid)
        Q = layers.squeeze(Q, axes=[1])
        return Q

```

#### ï¼ˆ2ï¼‰Algorithm
* `Algorithm` å®šä¹‰äº†å…·ä½“çš„ç®—æ³•æ¥æ›´æ–°å‰å‘ç½‘ç»œ(`Model`)ï¼Œä¹Ÿå°±æ˜¯é€šè¿‡å®šä¹‰æŸå¤±å‡½æ•°æ¥æ›´æ–°`Model`ï¼Œå’Œç®—æ³•ç›¸å…³çš„è®¡ç®—éƒ½æ”¾åœ¨`algorithm`ä¸­ã€‚


```python
# from parl.algorithms import DDPG # ä¹Ÿå¯ä»¥ç›´æ¥ä»parlåº“ä¸­å¿«é€Ÿå¼•å…¥DDPGç®—æ³•ï¼Œæ— éœ€è‡ªå·±é‡æ–°å†™ç®—æ³•

class DDPG(parl.Algorithm):
    def __init__(self,
                 model,
                 gamma=None,
                 tau=None,
                 actor_lr=None,
                 critic_lr=None):
        """  DDPG algorithm
        
        Args:
            model (parl.Model): actor and critic çš„å‰å‘ç½‘ç»œ.
                                model å¿…é¡»å®ç° get_actor_params() æ–¹æ³•.
            gamma (float): rewardçš„è¡°å‡å› å­.
            tau (float): self.target_model è·Ÿ self.model åŒæ­¥å‚æ•° çš„ è½¯æ›´æ–°å‚æ•°
            actor_lr (float): actor çš„å­¦ä¹ ç‡
            critic_lr (float): critic çš„å­¦ä¹ ç‡
        """
        assert isinstance(gamma, float)
        assert isinstance(tau, float)
        assert isinstance(actor_lr, float)
        assert isinstance(critic_lr, float)
        self.gamma = gamma
        self.tau = tau
        self.actor_lr = actor_lr
        self.critic_lr = critic_lr

        self.model = model
        self.target_model = deepcopy(model)

    def predict(self, obs):
        """ ä½¿ç”¨ self.model çš„ actor model æ¥é¢„æµ‹åŠ¨ä½œ
        """
        return self.model.policy(obs)

    def learn(self, obs, action, reward, next_obs, terminal):
        """ ç”¨DDPGç®—æ³•æ›´æ–° actor å’Œ critic
        """
        actor_cost = self._actor_learn(obs)
        critic_cost = self._critic_learn(obs, action, reward, next_obs,
                                         terminal)
        return actor_cost, critic_cost

    def _actor_learn(self, obs):
        action = self.model.policy(obs)
        Q = self.model.value(obs, action)
        cost = layers.reduce_mean(-1.0 * Q)
        optimizer = fluid.optimizer.AdamOptimizer(self.actor_lr)
        optimizer.minimize(cost, parameter_list=self.model.get_actor_params())
        return cost

    def _critic_learn(self, obs, action, reward, next_obs, terminal):
        next_action = self.target_model.policy(next_obs)
        next_Q = self.target_model.value(next_obs, next_action)

        terminal = layers.cast(terminal, dtype='float32')
        target_Q = reward + (1.0 - terminal) * self.gamma * next_Q
        target_Q.stop_gradient = True

        Q = self.model.value(obs, action)
        cost = layers.square_error_cost(Q, target_Q)
        cost = layers.reduce_mean(cost)
        optimizer = fluid.optimizer.AdamOptimizer(self.critic_lr)
        optimizer.minimize(cost)
        return cost

    def sync_target(self, decay=None, share_vars_parallel_executor=None):
        """ self.target_modelä»self.modelå¤åˆ¶å‚æ•°è¿‡æ¥ï¼Œå¯è®¾ç½®è½¯æ›´æ–°å‚æ•°
        """
        if decay is None:
            decay = 1.0 - self.tau
        self.model.sync_weights_to(
            self.target_model,
            decay=decay,
            share_vars_parallel_executor=share_vars_parallel_executor)

```

#### ï¼ˆ3ï¼‰Agent
* `Agent`è´Ÿè´£ç®—æ³•ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œåœ¨äº¤äº’è¿‡ç¨‹ä¸­æŠŠç”Ÿæˆçš„æ•°æ®æä¾›ç»™`Algorithm`æ¥æ›´æ–°æ¨¡å‹(`Model`)ï¼Œæ•°æ®çš„é¢„å¤„ç†æµç¨‹ä¹Ÿä¸€èˆ¬å®šä¹‰åœ¨è¿™é‡Œã€‚


```python
class Agent(parl.Agent):
    def __init__(self, algorithm, obs_dim, act_dim):
        assert isinstance(obs_dim, int)
        assert isinstance(act_dim, int)
        self.obs_dim = obs_dim
        self.act_dim = act_dim
        super(Agent, self).__init__(algorithm)

        # æ³¨æ„ï¼šæœ€å¼€å§‹å…ˆåŒæ­¥self.modelå’Œself.target_modelçš„å‚æ•°.
        self.alg.sync_target(decay=0)

    def build_program(self):
        self.pred_program = fluid.Program()
        self.learn_program = fluid.Program()

        with fluid.program_guard(self.pred_program):
            obs = layers.data(
                name='obs', shape=[self.obs_dim], dtype='float32')
            self.pred_act = self.alg.predict(obs)

        with fluid.program_guard(self.learn_program):
            obs = layers.data(
                name='obs', shape=[self.obs_dim], dtype='float32')
            act = layers.data(
                name='act', shape=[self.act_dim], dtype='float32')
            reward = layers.data(name='reward', shape=[], dtype='float32')
            next_obs = layers.data(
                name='next_obs', shape=[self.obs_dim], dtype='float32')
            terminal = layers.data(name='terminal', shape=[], dtype='bool')
            _, self.critic_cost = self.alg.learn(obs, act, reward, next_obs,
                                                 terminal)

    def predict(self, obs):
        obs = np.expand_dims(obs, axis=0)
        act = self.fluid_executor.run(
            self.pred_program, feed={'obs': obs},
            fetch_list=[self.pred_act])[0]
        act = np.squeeze(act)
        return act

    def learn(self, obs, act, reward, next_obs, terminal):
        feed = {
            'obs': obs,
            'act': act,
            'reward': reward,
            'next_obs': next_obs,
            'terminal': terminal
        }
        critic_cost = self.fluid_executor.run(
            self.learn_program, feed=feed, fetch_list=[self.critic_cost])[0]
        self.alg.sync_target()
        return critic_cost

```

### env.py
#### è¿ç»­æ§åˆ¶ç‰ˆæœ¬çš„CartPoleç¯å¢ƒ 
* è¯¥ç¯å¢ƒä»£ç ä¸ç®—æ³•æ— å…³ï¼Œå¯å¿½ç•¥ä¸çœ‹


```python
# env.py
# Continuous version of Cartpole

import math
import gym
from gym import spaces
from gym.utils import seeding
import numpy as np


class ContinuousCartPoleEnv(gym.Env):
    metadata = {
        'render.modes': ['human', 'rgb_array'],
        'video.frames_per_second': 50
    }

    def __init__(self):
        self.gravity = 9.8
        self.masscart = 1.0
        self.masspole = 0.1
        self.total_mass = (self.masspole + self.masscart)
        self.length = 0.5  # actually half the pole's length
        self.polemass_length = (self.masspole * self.length)
        self.force_mag = 30.0
        self.tau = 0.02  # seconds between state updates
        self.min_action = -1.0
        self.max_action = 1.0

        # Angle at which to fail the episode
        self.theta_threshold_radians = 12 * 2 * math.pi / 360
        self.x_threshold = 2.4

        # Angle limit set to 2 * theta_threshold_radians so failing observation
        # is still within bounds
        high = np.array([
            self.x_threshold * 2,
            np.finfo(np.float32).max,
            self.theta_threshold_radians * 2,
            np.finfo(np.float32).max])

        self.action_space = spaces.Box(
            low=self.min_action,
            high=self.max_action,
            shape=(1,)
        )
        self.observation_space = spaces.Box(-high, high)

        self.seed()
        self.viewer = None
        self.state = None

        self.steps_beyond_done = None

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def stepPhysics(self, force):
        x, x_dot, theta, theta_dot = self.state
        costheta = math.cos(theta)
        sintheta = math.sin(theta)
        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
        thetaacc = (self.gravity * sintheta - costheta * temp) / \
            (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))
        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
        x = x + self.tau * x_dot
        x_dot = x_dot + self.tau * xacc
        theta = theta + self.tau * theta_dot
        theta_dot = theta_dot + self.tau * thetaacc
        return (x, x_dot, theta, theta_dot)

    def step(self, action):
        action = np.expand_dims(action, 0)
        assert self.action_space.contains(action), \
            "%r (%s) invalid" % (action, type(action))
        # Cast action to float to strip np trappings
        force = self.force_mag * float(action)
        self.state = self.stepPhysics(force)
        x, x_dot, theta, theta_dot = self.state
        done = x < -self.x_threshold \
            or x > self.x_threshold \
            or theta < -self.theta_threshold_radians \
            or theta > self.theta_threshold_radians
        done = bool(done)

        if not done:
            reward = 1.0
        elif self.steps_beyond_done is None:
            # Pole just fell!
            self.steps_beyond_done = 0
            reward = 1.0
        else:
            if self.steps_beyond_done == 0:
                gym.logger.warn("""
You are calling 'step()' even though this environment has already returned
done = True. You should always call 'reset()' once you receive 'done = True'
Any further steps are undefined behavior.
                """)
            self.steps_beyond_done += 1
            reward = 0.0

        return np.array(self.state), reward, done, {}

    def reset(self):
        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
        self.steps_beyond_done = None
        return np.array(self.state)

    def render(self, mode='human'):
        screen_width = 600
        screen_height = 400

        world_width = self.x_threshold * 2
        scale = screen_width /world_width
        carty = 100  # TOP OF CART
        polewidth = 10.0
        polelen = scale * 1.0
        cartwidth = 50.0
        cartheight = 30.0

        if self.viewer is None:
            from gym.envs.classic_control import rendering
            self.viewer = rendering.Viewer(screen_width, screen_height)
            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
            axleoffset = cartheight / 4.0
            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
            self.carttrans = rendering.Transform()
            cart.add_attr(self.carttrans)
            self.viewer.add_geom(cart)
            l, r, t, b = -polewidth / 2, polewidth / 2, polelen-polewidth / 2, -polewidth / 2
            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
            pole.set_color(.8, .6, .4)
            self.poletrans = rendering.Transform(translation=(0, axleoffset))
            pole.add_attr(self.poletrans)
            pole.add_attr(self.carttrans)
            self.viewer.add_geom(pole)
            self.axle = rendering.make_circle(polewidth / 2)
            self.axle.add_attr(self.poletrans)
            self.axle.add_attr(self.carttrans)
            self.axle.set_color(.5, .5, .8)
            self.viewer.add_geom(self.axle)
            self.track = rendering.Line((0, carty), (screen_width, carty))
            self.track.set_color(0, 0, 0)
            self.viewer.add_geom(self.track)

        if self.state is None:
            return None

        x = self.state
        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
        self.carttrans.set_translation(cartx, carty)
        self.poletrans.set_rotation(-x[2])

        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))

    def close(self):
        if self.viewer:
            self.viewer.close()

```

## replay_memory.py
#### ç»éªŒæ±  ReplayMemory
* ä¸`DQN`çš„`replay_mamory.py`ä»£ç ä¸€è‡´


```python
# replay_memory.py
import random
import collections
import numpy as np


class ReplayMemory(object):
    def __init__(self, max_size):
        self.buffer = collections.deque(maxlen=max_size)

    def append(self, exp):
        self.buffer.append(exp)

    def sample(self, batch_size):
        mini_batch = random.sample(self.buffer, batch_size)
        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = [], [], [], [], []

        for experience in mini_batch:
            s, a, r, s_p, done = experience
            obs_batch.append(s)
            action_batch.append(a)
            reward_batch.append(r)
            next_obs_batch.append(s_p)
            done_batch.append(done)

        return np.array(obs_batch).astype('float32'), \
            np.array(action_batch).astype('float32'), np.array(reward_batch).astype('float32'),\
            np.array(next_obs_batch).astype('float32'), np.array(done_batch).astype('float32')

    def __len__(self):
        return len(self.buffer)

```

### Step5 Training && Testï¼ˆè®­ç»ƒ&&æµ‹è¯•ï¼‰


```python
def run_episode(agent, env, rpm):
    obs = env.reset()
    total_reward = 0
    steps = 0
    while True:
        steps += 1
        batch_obs = np.expand_dims(obs, axis=0)
        action = agent.predict(batch_obs.astype('float32'))

        # å¢åŠ æ¢ç´¢æ‰°åŠ¨, è¾“å‡ºé™åˆ¶åœ¨ [-1.0, 1.0] èŒƒå›´å†…
        action = np.clip(np.random.normal(action, NOISE), -1.0, 1.0)

        next_obs, reward, done, info = env.step(action)

        action = [action]  # æ–¹ä¾¿å­˜å…¥replaymemory
        rpm.append((obs, action, REWARD_SCALE * reward, next_obs, done))

        if len(rpm) > MEMORY_WARMUP_SIZE and (steps % 5) == 0:
            (batch_obs, batch_action, batch_reward, batch_next_obs,
             batch_done) = rpm.sample(BATCH_SIZE)
            agent.learn(batch_obs, batch_action, batch_reward, batch_next_obs,
                        batch_done)

        obs = next_obs
        total_reward += reward

        if done or steps >= 200:
            break
    return total_reward


def evaluate(env, agent, render=False):
    eval_reward = []
    for i in range(5):
        obs = env.reset()
        total_reward = 0
        steps = 0
        while True:
            batch_obs = np.expand_dims(obs, axis=0)
            action = agent.predict(batch_obs.astype('float32'))
            action = np.clip(action, -1.0, 1.0)

            steps += 1
            next_obs, reward, done, info = env.step(action)

            obs = next_obs
            total_reward += reward

            if render:
                env.render()
            if done or steps >= 200:
                break
        eval_reward.append(total_reward)
    return np.mean(eval_reward)

```

### Step6 åˆ›å»ºç¯å¢ƒå’ŒAgentï¼Œåˆ›å»ºç»éªŒæ± ï¼Œå¯åŠ¨è®­ç»ƒï¼Œä¿å­˜æ¨¡å‹


```python
# åˆ›å»ºç¯å¢ƒ
env = ContinuousCartPoleEnv()

obs_dim = env.observation_space.shape[0]
act_dim = env.action_space.shape[0]

# ä½¿ç”¨PARLæ¡†æ¶åˆ›å»ºagent
model = Model(act_dim)
algorithm = DDPG(
    model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)
agent = Agent(algorithm, obs_dim, act_dim)

# åˆ›å»ºç»éªŒæ± 
rpm = ReplayMemory(MEMORY_SIZE)
# å¾€ç»éªŒæ± ä¸­é¢„å­˜æ•°æ®
while len(rpm) < MEMORY_WARMUP_SIZE:
    run_episode(agent, env, rpm)

episode = 0
while episode < TRAIN_EPISODE:
    for i in range(50):
        total_reward = run_episode(agent, env, rpm)
        episode += 1

    eval_reward = evaluate(env, agent, render=False)
    logger.info('episode:{}    test_reward:{}'.format(
        episode, eval_reward))
```

    [32m[06-11 16:26:59 MainThread @machine_info.py:84][0m Cannot find available GPU devices, using CPU now.
    [32m[06-11 16:26:59 MainThread @machine_info.py:84][0m Cannot find available GPU devices, using CPU now.
    [32m[06-11 16:26:59 MainThread @machine_info.py:84][0m Cannot find available GPU devices, using CPU now.
    [32m[06-11 16:28:44 MainThread @machine_info.py:84][0m Cannot find available GPU devices, using CPU now.
    [32m[06-11 16:28:46 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:50    test_reward:6.6
    [32m[06-11 16:28:48 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:100    test_reward:5.8
    [32m[06-11 16:28:49 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:150    test_reward:6.0
    [32m[06-11 16:28:50 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:200    test_reward:6.0
    [32m[06-11 16:28:52 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:250    test_reward:5.8
    [32m[06-11 16:28:53 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:300    test_reward:5.8
    [32m[06-11 16:28:55 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:350    test_reward:5.8
    [32m[06-11 16:28:56 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:400    test_reward:6.2
    [32m[06-11 16:28:58 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:450    test_reward:5.8
    [32m[06-11 16:28:59 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:500    test_reward:6.0
    [32m[06-11 16:29:01 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:550    test_reward:5.8
    [32m[06-11 16:29:02 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:600    test_reward:5.8
    [32m[06-11 16:29:04 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:650    test_reward:5.8
    [32m[06-11 16:29:05 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:700    test_reward:5.8
    [32m[06-11 16:29:06 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:750    test_reward:5.8
    [32m[06-11 16:29:08 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:800    test_reward:5.8
    [32m[06-11 16:29:09 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:850    test_reward:5.8
    [32m[06-11 16:29:11 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:900    test_reward:6.0
    [32m[06-11 16:29:12 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:950    test_reward:5.8
    [32m[06-11 16:29:14 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1000    test_reward:6.0
    [32m[06-11 16:29:15 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1050    test_reward:6.0
    [32m[06-11 16:29:17 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1100    test_reward:5.6
    [32m[06-11 16:29:18 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1150    test_reward:6.2
    [32m[06-11 16:29:19 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1200    test_reward:5.8
    [32m[06-11 16:29:21 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1250    test_reward:6.0
    [32m[06-11 16:29:22 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1300    test_reward:6.0
    [32m[06-11 16:29:24 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1350    test_reward:5.6
    [32m[06-11 16:29:25 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1400    test_reward:5.8
    [32m[06-11 16:29:27 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1450    test_reward:6.0
    [32m[06-11 16:29:28 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1500    test_reward:5.8
    [32m[06-11 16:29:30 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1550    test_reward:6.0
    [32m[06-11 16:29:31 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1600    test_reward:5.8
    [32m[06-11 16:29:32 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1650    test_reward:6.0
    [32m[06-11 16:29:34 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1700    test_reward:5.6
    [32m[06-11 16:29:35 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1750    test_reward:6.0
    [32m[06-11 16:29:37 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1800    test_reward:6.0
    [32m[06-11 16:29:39 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1850    test_reward:5.8
    [32m[06-11 16:29:40 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1900    test_reward:6.0
    [32m[06-11 16:29:42 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:1950    test_reward:5.6
    [32m[06-11 16:29:43 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2000    test_reward:5.8
    [32m[06-11 16:29:45 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2050    test_reward:6.0
    [32m[06-11 16:29:46 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2100    test_reward:5.8
    [32m[06-11 16:29:48 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2150    test_reward:5.6
    [32m[06-11 16:29:49 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2200    test_reward:5.6
    [32m[06-11 16:29:50 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2250    test_reward:6.0
    [32m[06-11 16:29:52 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2300    test_reward:5.8
    [32m[06-11 16:29:53 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2350    test_reward:6.0
    [32m[06-11 16:29:55 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2400    test_reward:5.8
    [32m[06-11 16:29:56 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2450    test_reward:6.0
    [32m[06-11 16:29:58 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2500    test_reward:6.0
    [32m[06-11 16:29:59 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2550    test_reward:5.8
    [32m[06-11 16:30:01 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2600    test_reward:5.8
    [32m[06-11 16:30:02 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2650    test_reward:5.8
    [32m[06-11 16:30:04 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2700    test_reward:5.4
    [32m[06-11 16:30:05 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2750    test_reward:6.0
    [32m[06-11 16:30:07 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2800    test_reward:6.0
    [32m[06-11 16:30:08 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2850    test_reward:6.0
    [32m[06-11 16:30:10 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2900    test_reward:5.8
    [32m[06-11 16:30:11 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:2950    test_reward:6.2
    [32m[06-11 16:30:13 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3000    test_reward:5.8
    [32m[06-11 16:30:14 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3050    test_reward:5.8
    [32m[06-11 16:30:16 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3100    test_reward:6.0
    [32m[06-11 16:30:17 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3150    test_reward:5.8
    [32m[06-11 16:30:19 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3200    test_reward:5.8
    [32m[06-11 16:30:20 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3250    test_reward:5.8
    [32m[06-11 16:30:22 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3300    test_reward:6.0
    [32m[06-11 16:30:23 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3350    test_reward:5.8
    [32m[06-11 16:30:25 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3400    test_reward:5.6
    [32m[06-11 16:30:26 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3450    test_reward:6.0
    [32m[06-11 16:30:28 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3500    test_reward:5.8
    [32m[06-11 16:30:29 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3550    test_reward:6.0
    [32m[06-11 16:30:31 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3600    test_reward:5.8
    [32m[06-11 16:30:32 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3650    test_reward:5.8
    [32m[06-11 16:30:34 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3700    test_reward:6.0
    [32m[06-11 16:30:36 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3750    test_reward:6.0
    [32m[06-11 16:30:37 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3800    test_reward:6.0
    [32m[06-11 16:30:39 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3850    test_reward:5.6
    [32m[06-11 16:30:40 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3900    test_reward:6.0
    [32m[06-11 16:30:42 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:3950    test_reward:5.6
    [32m[06-11 16:30:43 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4000    test_reward:5.6
    [32m[06-11 16:30:50 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4050    test_reward:56.0
    [32m[06-11 16:31:19 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4100    test_reward:200.0
    [32m[06-11 16:32:11 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4150    test_reward:200.0
    [32m[06-11 16:33:08 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4200    test_reward:200.0
    [32m[06-11 16:34:06 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4250    test_reward:200.0
    [32m[06-11 16:35:05 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4300    test_reward:200.0
    [32m[06-11 16:36:04 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4350    test_reward:200.0
    [32m[06-11 16:37:04 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4400    test_reward:200.0
    [32m[06-11 16:38:03 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4450    test_reward:200.0
    [32m[06-11 16:39:02 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4500    test_reward:200.0
    [32m[06-11 16:40:03 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4550    test_reward:200.0
    [32m[06-11 16:40:58 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4600    test_reward:141.2
    [32m[06-11 16:41:50 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4650    test_reward:200.0
    [32m[06-11 16:42:53 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4700    test_reward:200.0
    [32m[06-11 16:43:54 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4750    test_reward:200.0
    [32m[06-11 16:44:55 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4800    test_reward:200.0
    [32m[06-11 16:45:57 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4850    test_reward:200.0
    [32m[06-11 16:46:57 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4900    test_reward:200.0
    [32m[06-11 16:47:58 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:4950    test_reward:200.0
    [32m[06-11 16:48:57 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5000    test_reward:200.0
    [32m[06-11 16:49:58 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5050    test_reward:200.0
    [32m[06-11 16:50:56 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5100    test_reward:200.0
    [32m[06-11 16:52:03 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5150    test_reward:200.0
    [32m[06-11 16:53:07 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5200    test_reward:200.0
    [32m[06-11 16:54:15 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5250    test_reward:200.0
    [32m[06-11 16:55:18 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5300    test_reward:200.0
    [32m[06-11 16:56:28 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5350    test_reward:200.0
    [32m[06-11 16:57:34 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5400    test_reward:198.4
    [32m[06-11 16:58:43 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5450    test_reward:188.4
    [32m[06-11 16:59:51 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5500    test_reward:198.8
    [32m[06-11 17:00:58 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5550    test_reward:200.0
    [32m[06-11 17:02:10 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5600    test_reward:200.0
    [32m[06-11 17:03:18 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5650    test_reward:200.0
    [32m[06-11 17:04:31 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5700    test_reward:200.0
    [32m[06-11 17:05:44 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5750    test_reward:200.0
    [32m[06-11 17:06:53 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5800    test_reward:200.0
    [32m[06-11 17:08:09 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5850    test_reward:200.0
    [32m[06-11 17:09:23 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5900    test_reward:200.0
    [32m[06-11 17:10:32 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:5950    test_reward:200.0
    [32m[06-11 17:11:52 MainThread @<ipython-input-10-f4e0a11a1638>:27][0m episode:6000    test_reward:200.0

